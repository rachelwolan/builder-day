# AI Product Org Maturity Model (AIPOM)
## A Framework for AI-Native Product Organizations

**For:** Product Management, Product Design, Data Science, and User Research Teams

This maturity model helps product organizations assess and advance their capability to build, experiment, and learn using AI-powered tools. It measures how effectively teams leverage AI to accelerate product development, from initial concept to validated insights.

---

## Format Adaptation Strategy

### Core Structure Elements to Preserve from AEOMM

1. **Maturity Levels** (typically 4-5 levels)
   - Level 1: Initial/Ad-hoc
   - Level 2: Developing/Managed
   - Level 3: Defined/Established
   - Level 4: Advanced/Optimized
   - Level 5: (Optional) Leading/Innovating

2. **Key Dimensions/Pillars** - Areas of assessment
   - Each dimension should have clear progression across levels
   - Visual representation (matrix/heatmap) showing maturity across dimensions

3. **Assessment Criteria** - Specific, measurable indicators for each level

4. **Visual Design Elements**
   - Color coding by maturity level
   - Clear typography hierarchy
   - Icons or visual indicators
   - Progress indicators

---

## Core Dimensions for Product Organizations

The AI Product Org Maturity Model assesses five key dimensions that determine an organization's ability to build AI-native products:

1. **Prototyping & Design Velocity** - How quickly Product and Design teams can go from idea to interactive prototype
2. **Data & Insights Automation** - How effectively Data Science and Research teams use AI to accelerate analysis and discovery
3. **Tool Fluency & Infrastructure** - The depth and breadth of AI tool adoption across all product teams
4. **Cross-Functional Builder Culture** - How well PM, Design, Data, and Research collaborate using AI tools
5. **Learning & Experimentation** - The organization's ability to learn from experiments and continuously improve

Each dimension is assessed across five maturity levels, with specific indicators for Product Management, Product Design, Data Science, and User Research roles.

---

## Maturity Levels

All dimensions are assessed across five levels:

- **Level 1: Initial** - Ad-hoc, individual experimentation
- **Level 2: Developing** - Some teams experimenting, early standardization
- **Level 3: Defined** - Standardized practices, integrated workflows
- **Level 4: Advanced** - Optimized usage, continuous improvement
- **Level 5: Leading** - Innovation and thought leadership

---

## Dimension 1: Prototyping & Design Velocity

**Definition:** The speed and quality with which Product and Design teams can transform ideas into interactive prototypes using AI-powered tools.

### Level 1: Initial
**State:** Prototypes are rare, time-intensive, and primarily created by specialists.

**Product Management:**
- Prototypes are requested from Design/Engineering, not created by PMs
- No AI tools used for rapid concept exploration
- Product concepts communicated via documents and static mockups

**Product Design:**
- Prototypes take days or weeks to create
- Limited use of AI design tools (Figma Make, etc.)
- High-fidelity prototypes require significant design time
- Prototypes are polished before sharing

**Indicators:**
- <10% of product concepts have prototypes
- Average prototype creation time: 1-2 weeks
- Prototypes created only by Design team
- No AI-assisted prototyping tools in use

### Level 2: Developing
**State:** Some teams experimenting with AI prototyping tools, early wins visible.

**Product Management:**
- PMs creating low-fidelity prototypes for concept validation
- Using AI tools (Cursor, Figma Make) for quick explorations
- Prototypes used in early discovery conversations

**Product Design:**
- Designers experimenting with Figma Make and AI code assistants
- Some prototypes created in hours instead of days
- Mix of low and high-fidelity prototypes based on need
- AI tools used for initial explorations, then refined manually

**Indicators:**
- 25-40% of product concepts have prototypes
- Average prototype creation time: 2-5 days
- PMs creating 10-20% of prototypes
- Basic training on AI prototyping tools available

### Level 3: Defined
**State:** Prototyping is standard practice, AI tools integrated into workflows.

**Product Management:**
- PMs regularly create low-to-medium fidelity prototypes
- Prototypes included in standard project kickoff templates
- AI tools used for opportunity exploration and solution ideation
- Prototypes shared early in discovery phase

**Product Design:**
- Designers proficient with AI prototyping tools (Figma Make, Cursor)
- Prototype fidelity matched to phase (discovery = low, design = medium, build = high)
- AI tools used throughout design process, not just exploration
- Prototypes updated frequently with daily/weekly iterations

**Indicators:**
- 60-75% of product concepts have prototypes
- Average prototype creation time: 4-8 hours for low-fidelity, 1-2 days for medium
- PMs creating 30-40% of prototypes
- Quarterly training on prototyping tools and best practices
- Prototypes integrated into standard project workflows

### Level 4: Advanced
**State:** High-velocity prototyping, optimized workflows, measurable impact.

**Product Management:**
- PMs create prototypes for most concepts before formal design work
- Prototypes used for stakeholder alignment and user validation
- AI tools used to explore multiple solution directions quickly
- Prototype-to-decision time measured and optimized

**Product Design:**
- Designers create prototypes in hours, not days
- AI tools used for rapid iteration and variant exploration
- Custom workflows and automations for common prototype patterns
- Prototypes serve as primary communication artifact
- Design system components integrated into AI-generated prototypes

**Indicators:**
- 85%+ of product concepts have prototypes
- Average prototype creation time: 1-4 hours for low-fidelity, 4-8 hours for medium
- PMs creating 50%+ of initial prototypes
- Prototype usage tracked and measured
- Custom templates and workflows for common patterns
- Prototype-to-value metrics established

### Level 5: Leading
**State:** Prototyping is core to product development, continuous innovation.

**Product Management:**
- PMs are prototyping experts, teaching others
- Prototypes used for all major decisions
- AI tools used to explore edge cases and complex scenarios
- Contributing to tool development and best practices
- PMs submitting pull requests and contributing code directly
- PMs generating Python notebooks for data analysis and prototyping

**Product Design:**
- Designers pioneering new prototyping techniques
- AI tools used to create production-quality prototypes
- Sharing prototyping best practices externally
- Prototypes often become production code or inform architecture
- Product Designers submitting pull requests and contributing code
- Designers generating Python notebooks for analysis and automation

**Indicators:**
- 95%+ of concepts prototyped before formal design
- Average prototype creation time: <1 hour for low-fidelity
- PMs creating 60%+ of initial prototypes
- PMs and Product Designers submitting pull requests regularly
- PMs and Product Designers generating Python notebooks for analysis
- Thought leadership on prototyping (blog posts, talks, tool contributions)
- Prototypes directly inform or become production features

---

## Dimension 2: Data & Insights Automation

**Definition:** How effectively Data Science and User Research teams use AI to accelerate analysis, automate synthesis, and generate actionable insights.

### Level 1: Initial
**State:** Manual analysis, limited AI assistance, insights take weeks.

**Data Science:**
- All analysis done manually with traditional tools
- No AI-assisted data exploration or pattern recognition
- Reports and dashboards created manually
- Limited automation in data pipelines

**User Research:**
- Research synthesis done manually
- Interview transcripts analyzed by hand
- No AI tools for pattern recognition in qualitative data
- Insights compiled in documents and presentations

**Indicators:**
- Average time from data collection to insights: 2-4 weeks
- <10% of analysis uses AI tools
- No automated data synthesis
- Research reports created manually

### Level 2: Developing
**State:** Early experimentation with AI analysis tools, some automation.

**Data Science:**
- Data scientists experimenting with AI code assistants (Cursor) for analysis
- Some automated data exploration using AI
- AI used to generate initial hypotheses from data
- Basic automation in data cleaning and transformation

**User Research:**
- Researchers using AI to transcribe and summarize interviews
- AI tools used for initial theme identification
- Some automated synthesis of research findings
- AI-assisted report generation

**Indicators:**
- Average time from data to insights: 1-2 weeks
- 25-40% of analysis uses AI tools
- Basic AI transcription and summarization in use
- Initial automation of repetitive analysis tasks

### Level 3: Defined
**State:** AI tools integrated into standard analysis workflows.

**Data Science:**
- AI code assistants used for most data analysis work
- Automated pattern recognition in datasets
- AI used to generate analysis scripts and visualizations
- Standard workflows include AI-assisted exploration
- AI helps identify anomalies and trends

**User Research:**
- AI transcription and summarization standard for all interviews
- AI tools used for theme extraction and pattern recognition
- Automated synthesis of research findings across studies
- AI-assisted report generation with human review
- Research insights database maintained with AI assistance

**Indicators:**
- Average time from data to insights: 3-7 days
- 60-75% of analysis uses AI tools
- All interviews transcribed and summarized with AI
- Standard research workflows include AI tools
- Quarterly training on AI analysis tools

### Level 4: Advanced
**State:** High automation, optimized workflows, measurable impact on speed.

**Data Science:**
- AI tools used for advanced analysis and modeling
- Custom AI workflows for common analysis patterns
- Automated insight generation and report creation
- AI used to identify opportunities and anomalies proactively
- Analysis velocity measured and optimized

**User Research:**
- AI used for real-time research synthesis
- AI tools identify patterns across multiple research studies
- Automated research insight extraction and sharing
- AI-assisted user journey mapping and persona development
- Research insights automatically integrated into product decisions

**Indicators:**
- Average time from data to insights: 1-3 days
- 85%+ of analysis uses AI tools
- Custom AI workflows for research synthesis
- Research insights automatically surfaced to product teams
- Analysis velocity metrics tracked and improved

### Level 5: Leading
**State:** AI-native analysis, continuous innovation, thought leadership.

**Data Science:**
- Data scientists pioneering new AI analysis techniques
- Custom AI models and tools developed internally
- AI used for predictive insights and proactive recommendations
- Contributing to AI analysis tool development
- Sharing best practices externally
- Building reusable analysis frameworks and libraries used across the org
- Creating internal tools and platforms that democratize data analysis
- Contributing to open source AI analysis tools

**User Research:**
- Researchers using AI for advanced qualitative analysis
- AI-powered research synthesis across multiple data sources
- Automated insight generation and recommendation systems
- Thought leadership on AI-assisted research
- Research insights drive product strategy proactively
- Building reusable research frameworks and automation pipelines
- Creating internal tools that enable self-service research analysis
- Contributing to open source research tools and methodologies

**Indicators:**
- Average time from data to insights: <1 day for standard analysis
- 95%+ of analysis uses AI tools
- Custom AI tools developed for research needs
- Reusable frameworks and libraries created by Data Science/Research teams
- Internal tools built that enable other teams to do analysis
- Thought leadership (blog posts, talks, tool contributions)
- Research insights automatically inform product roadmap
- Open source contributions to AI analysis or research tools

---

## Dimension 3: Tool Fluency & Infrastructure

**Definition:** The depth and breadth of AI tool adoption, proficiency, and infrastructure support across Product, Design, Data, and Research teams.

### Level 1: Initial
**State:** Ad-hoc tool usage, no standardization, limited access.

**All Teams:**
- Individual experimentation with various AI tools
- No standardized toolset
- Tools chosen individually, not organizationally
- Limited budget or procurement process
- No training or support structure

**Indicators:**
- <10% of product org uses AI tools regularly
- No approved tool list
- No tool budget or procurement process
- No training programs
- Tools accessed individually (personal accounts)

### Level 2: Developing
**State:** Some teams experimenting, early standardization beginning.

**All Teams:**
- Pilot programs in select teams (e.g., one product team, design team)
- Basic tool evaluation process
- Initial vendor relationships forming
- Some training sessions available
- Tool access managed at team level

**Indicators:**
- 25-40% adoption in key teams
- Initial tool evaluation framework
- Basic training sessions offered (quarterly or ad-hoc)
- Some tools procured organizationally
- Tool champions emerging in each discipline

### Level 3: Defined
**State:** Standardized toolset, integrated workflows, regular training.

**All Teams:**
- Approved tool stack established for each discipline
- Tools integrated into standard project templates
- Regular training programs (quarterly minimum)
- Tool usage tracked and measured
- Clear procurement and access process
- Tool documentation and best practices available

**Product-Specific Tools:**
- AI code editors (Cursor) for prototyping
- AI design tools (Figma Make) for rapid design
- AI analysis platforms for data insights

**Indicators:**
- 60-75% adoption across product org
- Standardized tool stack per discipline
- Quarterly training cadence
- Tools in standard project kickoff templates
- Tool usage metrics tracked
- New team members onboarded to tools

### Level 4: Advanced
**State:** High proficiency, custom integrations, optimized usage.

**All Teams:**
- High proficiency across all teams
- Custom tool integrations and workflows
- Advanced use cases and patterns documented
- Tool ROI measured and optimized
- Continuous improvement in tool usage
- Internal tool champions and communities

**Product-Specific Tools:**
- Custom workflows built on top of AI tools
- Tool integrations with design systems and data platforms
- Advanced prompting and automation patterns
- Tool usage analytics and optimization

**Indicators:**
- 85%+ adoption with high proficiency
- Custom workflows and automations
- Tool ROI measured and reported
- Internal tool communities and knowledge sharing
- Advanced training and certification programs
- Tools integrated with core product infrastructure

### Level 5: Leading
**State:** Innovation, thought leadership, tool development.

**All Teams:**
- Contributing to tool development and improvement
- Sharing best practices externally
- Pioneering new use cases and patterns
- Tool partnerships and co-development
- Internal tool development where needed
- Industry thought leadership

**Indicators:**
- 95%+ adoption with expert-level proficiency
- Tool partnerships and co-development
- Industry speaking/thought leadership
- Internal tool development projects
- Open source contributions or tool improvements
- Tool usage drives competitive advantage

---

## Dimension 4: Cross-Functional Builder Culture

**Definition:** How effectively Product Management, Design, Data Science, and User Research collaborate using AI tools to build, experiment, and learn together.

### Level 1: Initial
**State:** Teams work in silos, limited collaboration, no shared AI practices.

**Characteristics:**
- Each discipline works independently
- Limited cross-functional collaboration
- No shared AI tool usage or practices
- Handoffs between teams are formal and document-heavy
- Little shared understanding of AI capabilities

**Indicators:**
- Teams use different tools and processes
- Collaboration happens through formal handoffs
- No shared AI tool knowledge or practices
- Limited cross-functional prototyping or analysis
- Silos between PM, Design, Data, Research

### Level 2: Developing
**State:** Some cross-functional collaboration, early shared practices.

**Characteristics:**
- Some teams collaborating on AI-powered projects
- Early shared tool usage (e.g., PM and Design using Figma Make together)
- Informal knowledge sharing about AI tools
- Cross-functional workshops or training sessions
- Some joint prototyping or analysis projects

**Indicators:**
- 25-40% of projects involve cross-functional AI tool usage
- Some shared tool training sessions
- Informal communities forming around AI tools
- PM-Design or Data-Research collaboration increasing
- Early examples of cross-functional prototypes

### Level 3: Defined
**State:** Standard cross-functional workflows, shared practices, regular collaboration.

**Characteristics:**
- Cross-functional teams regularly use AI tools together
- Shared tool stack and practices across disciplines
- Standard workflows for cross-functional collaboration
- Regular cross-functional training and knowledge sharing
- Prototypes and analyses created collaboratively

**Product-Design Collaboration:**
- PMs and Designers co-creating prototypes
- Shared understanding of prototyping tools and practices
- Joint exploration of product concepts

**Data-Research Collaboration:**
- Data Scientists and Researchers using AI tools together
- Shared analysis workflows and insights
- Collaborative research synthesis

**Cross-Discipline Collaboration:**
- PM-Data collaboration on insights and analysis
- Design-Research collaboration on user understanding
- All disciplines contributing to product decisions using AI tools

**Indicators:**
- 60-75% of projects involve cross-functional AI collaboration
- Standard cross-functional workflows established
- Quarterly cross-functional training sessions
- Shared tool documentation and best practices
- Cross-functional AI tool communities active

### Level 4: Advanced
**State:** Deep collaboration, optimized workflows, measurable impact.

**Characteristics:**
- Cross-functional collaboration is the norm, not exception
- Optimized workflows for different collaboration patterns
- Custom tools and integrations for cross-functional work
- Measured impact of collaboration on product outcomes
- Continuous improvement in collaboration practices

**Product-Design Collaboration:**
- PMs and Designers seamlessly co-create throughout product development
- Shared prototyping workflows and templates
- Real-time collaboration on prototypes and concepts

**Data-Research Collaboration:**
- Data Scientists and Researchers work as integrated team
- Shared analysis platforms and workflows
- Automated synthesis of quantitative and qualitative insights

**Cross-Discipline Collaboration:**
- All disciplines contribute to product decisions using AI tools
- Cross-functional AI tool usage drives product strategy
- Collaboration velocity measured and optimized

**Indicators:**
- 85%+ of projects involve cross-functional AI collaboration
- Custom workflows for cross-functional patterns
- Collaboration metrics tracked and improved
- Cross-functional AI tool expertise across all teams
- Collaboration drives measurable product outcomes

### Level 5: Leading
**State:** Seamless collaboration, innovation, thought leadership.

**Characteristics:**
- Cross-functional collaboration is core to product development
- Teams innovate together using AI tools
- Sharing best practices externally
- Contributing to cross-functional AI tool development
- Collaboration practices drive competitive advantage

**Indicators:**
- 95%+ of projects involve cross-functional AI collaboration
- Thought leadership on cross-functional AI collaboration
- Custom tools developed for cross-functional needs
- Collaboration practices shared externally
- Cross-functional AI usage drives innovation

---

## Dimension 5: Learning & Experimentation

**Definition:** The organization's ability to learn from experiments, share knowledge, and continuously improve AI-native practices.

### Level 1: Initial
**State:** Limited experimentation, knowledge siloed, no systematic learning.

**Characteristics:**
- Experimentation is rare and ad-hoc
- Learnings not systematically captured or shared
- No framework for AI tool experimentation
- Limited post-mortems or retrospectives on AI usage
- Knowledge stays with individuals

**Indicators:**
- <10% of teams regularly experiment with AI tools
- No systematic capture of learnings
- Limited knowledge sharing about AI tools
- No experimentation framework or process
- Learnings not applied to future work

### Level 2: Developing
**State:** Some experimentation, early knowledge sharing, basic learning processes.

**Characteristics:**
- Some teams experimenting with AI tools
- Early knowledge sharing (informal, ad-hoc)
- Basic frameworks for experimentation (e.g., Builder Day events)
- Some capture of learnings and best practices
- Early communities forming around AI tools

**Indicators:**
- 25-40% of teams regularly experiment
- Informal knowledge sharing (Slack, ad-hoc sessions)
- Basic experimentation frameworks (Builder Day, hackathons)
- Some documentation of learnings
- Early communities and champions emerging

### Level 3: Defined
**State:** Regular experimentation, systematic learning, knowledge sharing processes.

**Characteristics:**
- Regular experimentation with AI tools (quarterly events, ongoing)
- Systematic knowledge sharing processes
- Learning captured and documented
- Best practices identified and shared
- Regular retrospectives and post-mortems on AI usage
- Communities and forums for knowledge sharing

**Indicators:**
- 60-75% of teams regularly experiment
- Quarterly experimentation events (e.g., Builder Day)
- Systematic knowledge sharing (documentation, forums, sessions)
- Best practices documented and accessible
- Regular learning reviews and retrospectives
- Active communities around AI tools

### Level 4: Advanced
**State:** Continuous experimentation, optimized learning, measurable improvement.

**Characteristics:**
- Continuous experimentation embedded in workflows
- Optimized learning processes and knowledge sharing
- Learnings systematically applied to improve practices
- Experimentation outcomes measured and tracked
- Innovation cycles accelerated by learning
- Custom tools and processes for knowledge sharing

**Indicators:**
- 85%+ of teams continuously experiment
- Experimentation integrated into standard workflows
- Learning metrics tracked and improved
- Knowledge sharing optimized and automated where possible
- Learnings drive measurable improvements in practices
- Innovation cycles accelerated

### Level 5: Leading
**State:** Learning-driven innovation, thought leadership, external sharing.

**Characteristics:**
- Experimentation drives innovation and competitive advantage
- Learning processes are best-in-class
- Sharing learnings externally (blog posts, talks, contributions)
- Contributing to industry knowledge and practices
- Learning culture attracts and retains talent
- Continuous innovation through experimentation

**Indicators:**
- 95%+ of teams continuously experiment and innovate
- Thought leadership on AI-native learning practices
- External sharing of learnings and best practices
- Learning culture recognized externally
- Experimentation drives measurable competitive advantage
- Innovation cycles fastest in industry

---

## Assessment Tool

### How to Use This Assessment

1. **Assess Each Dimension Independently** - Your organization may be at different levels across dimensions
2. **Involve Multiple Perspectives** - Get input from PM, Design, Data, and Research teams
3. **Be Honest** - The goal is to identify where you are, not where you want to be
4. **Focus on Evidence** - Use the indicators to guide your assessment, not just opinions

### Self-Assessment Questions

For each dimension, answer the questions for your current level, then check if you meet the criteria for the next level.

#### Dimension 1: Prototyping & Design Velocity

**Level 1 Assessment:**
- [ ] Are prototypes created for less than 10% of product concepts?
- [ ] Do prototypes typically take 1-2 weeks to create?
- [ ] Are prototypes created only by the Design team?
- [ ] Are no AI-assisted prototyping tools in use?

**Level 2 Assessment:**
- [ ] Are prototypes created for 25-40% of product concepts?
- [ ] Do prototypes typically take 2-5 days to create?
- [ ] Are PMs creating 10-20% of prototypes?
- [ ] Is basic training on AI prototyping tools available?

**Level 3 Assessment:**
- [ ] Are prototypes created for 60-75% of product concepts?
- [ ] Can low-fidelity prototypes be created in 4-8 hours?
- [ ] Are PMs creating 30-40% of prototypes?
- [ ] Is quarterly training on prototyping tools provided?
- [ ] Are prototypes integrated into standard project workflows?

**Level 4 Assessment:**
- [ ] Are prototypes created for 85%+ of product concepts?
- [ ] Can low-fidelity prototypes be created in 1-4 hours?
- [ ] Are PMs creating 50%+ of initial prototypes?
- [ ] Is prototype usage tracked and measured?
- [ ] Are custom templates and workflows available?

**Level 5 Assessment:**
- [ ] Are 95%+ of concepts prototyped before formal design?
- [ ] Can low-fidelity prototypes be created in under 1 hour?
- [ ] Are PMs creating 60%+ of initial prototypes?
- [ ] Are PMs and Product Designers submitting pull requests regularly?
- [ ] Are PMs and Product Designers generating Python notebooks for analysis?
- [ ] Is there thought leadership on prototyping (blog posts, talks)?
- [ ] Do prototypes directly inform or become production features?

#### Dimension 2: Data & Insights Automation

**Level 1 Assessment:**
- [ ] Does it take 2-4 weeks from data collection to insights?
- [ ] Is less than 10% of analysis using AI tools?
- [ ] Is there no automated data synthesis?
- [ ] Are research reports created manually?

**Level 2 Assessment:**
- [ ] Does it take 1-2 weeks from data to insights?
- [ ] Is 25-40% of analysis using AI tools?
- [ ] Is basic AI transcription and summarization in use?
- [ ] Is there initial automation of repetitive analysis tasks?

**Level 3 Assessment:**
- [ ] Does it take 3-7 days from data to insights?
- [ ] Is 60-75% of analysis using AI tools?
- [ ] Are all interviews transcribed and summarized with AI?
- [ ] Do standard research workflows include AI tools?
- [ ] Is quarterly training on AI analysis tools provided?

**Level 4 Assessment:**
- [ ] Does it take 1-3 days from data to insights?
- [ ] Is 85%+ of analysis using AI tools?
- [ ] Are custom AI workflows used for research synthesis?
- [ ] Are research insights automatically surfaced to product teams?
- [ ] Are analysis velocity metrics tracked and improved?

**Level 5 Assessment:**
- [ ] Does it take less than 1 day for standard analysis?
- [ ] Is 95%+ of analysis using AI tools?
- [ ] Are custom AI tools developed for research needs?
- [ ] Are reusable frameworks and libraries created by Data Science/Research teams?
- [ ] Are internal tools built that enable other teams to do analysis?
- [ ] Is there thought leadership (blog posts, talks, tool contributions)?
- [ ] Do research insights automatically inform product roadmap?
- [ ] Are there open source contributions to AI analysis or research tools?

#### Dimension 3: Tool Fluency & Infrastructure

**Level 1 Assessment:**
- [ ] Do less than 10% of product org use AI tools regularly?
- [ ] Is there no approved tool list?
- [ ] Is there no tool budget or procurement process?
- [ ] Are there no training programs?
- [ ] Are tools accessed individually (personal accounts)?

**Level 2 Assessment:**
- [ ] Is there 25-40% adoption in key teams?
- [ ] Is there an initial tool evaluation framework?
- [ ] Are basic training sessions offered (quarterly or ad-hoc)?
- [ ] Are some tools procured organizationally?
- [ ] Are tool champions emerging in each discipline?

**Level 3 Assessment:**
- [ ] Is there 60-75% adoption across product org?
- [ ] Is there a standardized tool stack per discipline?
- [ ] Is there a quarterly training cadence?
- [ ] Are tools in standard project kickoff templates?
- [ ] Are tool usage metrics tracked?
- [ ] Are new team members onboarded to tools?

**Level 4 Assessment:**
- [ ] Is there 85%+ adoption with high proficiency?
- [ ] Are custom workflows and automations in use?
- [ ] Is tool ROI measured and reported?
- [ ] Are there internal tool communities and knowledge sharing?
- [ ] Are advanced training and certification programs available?
- [ ] Are tools integrated with core product infrastructure?

**Level 5 Assessment:**
- [ ] Is there 95%+ adoption with expert-level proficiency?
- [ ] Are there tool partnerships and co-development?
- [ ] Is there industry speaking/thought leadership?
- [ ] Are there internal tool development projects?
- [ ] Are there open source contributions or tool improvements?
- [ ] Does tool usage drive competitive advantage?

#### Dimension 4: Cross-Functional Builder Culture

**Level 1 Assessment:**
- [ ] Do teams use different tools and processes?
- [ ] Does collaboration happen through formal handoffs?
- [ ] Is there no shared AI tool knowledge or practices?
- [ ] Is there limited cross-functional prototyping or analysis?
- [ ] Are there silos between PM, Design, Data, Research?

**Level 2 Assessment:**
- [ ] Do 25-40% of projects involve cross-functional AI tool usage?
- [ ] Are there some shared tool training sessions?
- [ ] Are informal communities forming around AI tools?
- [ ] Is PM-Design or Data-Research collaboration increasing?
- [ ] Are there early examples of cross-functional prototypes?

**Level 3 Assessment:**
- [ ] Do 60-75% of projects involve cross-functional AI collaboration?
- [ ] Are standard cross-functional workflows established?
- [ ] Are quarterly cross-functional training sessions held?
- [ ] Is shared tool documentation and best practices available?
- [ ] Are cross-functional AI tool communities active?

**Level 4 Assessment:**
- [ ] Do 85%+ of projects involve cross-functional AI collaboration?
- [ ] Are custom workflows used for cross-functional patterns?
- [ ] Are collaboration metrics tracked and improved?
- [ ] Is there cross-functional AI tool expertise across all teams?
- [ ] Does collaboration drive measurable product outcomes?

**Level 5 Assessment:**
- [ ] Do 95%+ of projects involve cross-functional AI collaboration?
- [ ] Is there thought leadership on cross-functional AI collaboration?
- [ ] Are custom tools developed for cross-functional needs?
- [ ] Are collaboration practices shared externally?
- [ ] Does cross-functional AI usage drive innovation?

#### Dimension 5: Learning & Experimentation

**Level 1 Assessment:**
- [ ] Do less than 10% of teams regularly experiment with AI tools?
- [ ] Is there no systematic capture of learnings?
- [ ] Is there limited knowledge sharing about AI tools?
- [ ] Is there no experimentation framework or process?
- [ ] Are learnings not applied to future work?

**Level 2 Assessment:**
- [ ] Do 25-40% of teams regularly experiment?
- [ ] Is there informal knowledge sharing (Slack, ad-hoc sessions)?
- [ ] Are basic experimentation frameworks in place (Builder Day, hackathons)?
- [ ] Is there some documentation of learnings?
- [ ] Are early communities and champions emerging?

**Level 3 Assessment:**
- [ ] Do 60-75% of teams regularly experiment?
- [ ] Are quarterly experimentation events held (e.g., Builder Day)?
- [ ] Is there systematic knowledge sharing (documentation, forums, sessions)?
- [ ] Are best practices documented and accessible?
- [ ] Are regular learning reviews and retrospectives held?
- [ ] Are active communities around AI tools?

**Level 4 Assessment:**
- [ ] Do 85%+ of teams continuously experiment?
- [ ] Is experimentation integrated into standard workflows?
- [ ] Are learning metrics tracked and improved?
- [ ] Is knowledge sharing optimized and automated where possible?
- [ ] Do learnings drive measurable improvements in practices?
- [ ] Are innovation cycles accelerated?

**Level 5 Assessment:**
- [ ] Do 95%+ of teams continuously experiment and innovate?
- [ ] Is there thought leadership on AI-native learning practices?
- [ ] Is there external sharing of learnings and best practices?
- [ ] Is learning culture recognized externally?
- [ ] Does experimentation drive measurable competitive advantage?
- [ ] Are innovation cycles fastest in industry?

### Scoring Your Assessment

1. **For each dimension**, identify the highest level where you answer "Yes" to all questions
2. **Your maturity level** for that dimension is the highest level you fully meet
3. **If you meet some criteria** for a higher level but not all, note it as "approaching" that level
4. **Create a maturity profile** showing your level across all five dimensions

### Interpreting Your Results

**Uneven Maturity is Normal:** Most organizations will be at different levels across dimensions. This is expected and helps prioritize where to focus improvement efforts.

**Focus Areas:**
- **If Level 1-2 across most dimensions:** Focus on foundational tool adoption and training
- **If Level 2-3 across most dimensions:** Focus on standardization and workflow integration
- **If Level 3-4 across most dimensions:** Focus on optimization and advanced use cases
- **If Level 4-5 across most dimensions:** Focus on innovation and thought leadership

**Quick Wins:**
- Identify dimensions where you're close to the next level (e.g., Level 2 approaching Level 3)
- These are good candidates for focused improvement efforts
- Small investments can yield significant progress

---

## Roadmap: Advancing Your Maturity

### From Level 1 to Level 2

**Key Focus:** Start experimenting, build early wins

**Prototyping & Design Velocity:**
- Introduce AI prototyping tools (Figma Make, Cursor) to one product team
- Run pilot training session for PMs and Designers
- Set goal: Create 3-5 prototypes using AI tools in next quarter

**Data & Insights Automation:**
- Pilot AI transcription tools for user research interviews
- Experiment with AI code assistants (Cursor) for data analysis
- Set goal: Use AI tools for 25% of analysis work in next quarter

**Tool Fluency & Infrastructure:**
- Identify tool champions in each discipline
- Establish basic tool evaluation process
- Procure organizational licenses for 2-3 key tools

**Cross-Functional Builder Culture:**
- Run one cross-functional workshop on AI tools
- Create shared Slack channel for AI tool discussions
- Set goal: One cross-functional project using AI tools

**Learning & Experimentation:**
- Plan first Builder Day or hackathon event
- Create basic documentation template for learnings
- Set goal: Capture learnings from 3-5 experiments

### From Level 2 to Level 3

**Key Focus:** Standardize practices, integrate workflows

**Prototyping & Design Velocity:**
- Establish standard prototyping tool stack
- Integrate prototypes into project kickoff templates
- Set quarterly training cadence
- Set goal: 60% of concepts have prototypes, PMs create 30%

**Data & Insights Automation:**
- Standardize AI tools for all interviews and analysis
- Create standard workflows for AI-assisted analysis
- Set goal: 60% of analysis uses AI tools, 3-7 day insight cycle

**Tool Fluency & Infrastructure:**
- Finalize approved tool stack per discipline
- Establish quarterly training program
- Create tool documentation and best practices
- Set goal: 60% adoption across product org

**Cross-Functional Builder Culture:**
- Establish standard cross-functional workflows
- Create shared tool documentation
- Set quarterly cross-functional training
- Set goal: 60% of projects involve cross-functional AI collaboration

**Learning & Experimentation:**
- Establish quarterly Builder Day events
- Create systematic knowledge sharing process
- Document best practices
- Set goal: 60% of teams regularly experiment

### From Level 3 to Level 4

**Key Focus:** Optimize usage, measure impact

**Prototyping & Design Velocity:**
- Create custom templates and workflows
- Measure prototype-to-decision time
- Optimize based on metrics
- Set goal: 85% of concepts prototyped, 1-4 hour creation time

**Data & Insights Automation:**
- Create custom AI workflows for common patterns
- Automate insight surfacing to product teams
- Measure analysis velocity
- Set goal: 85% of analysis uses AI, 1-3 day insight cycle

**Tool Fluency & Infrastructure:**
- Measure and optimize tool ROI
- Create custom integrations and workflows
- Build internal tool communities
- Set goal: 85% adoption with high proficiency

**Cross-Functional Builder Culture:**
- Optimize collaboration workflows
- Measure collaboration impact on outcomes
- Create custom tools for cross-functional work
- Set goal: 85% of projects involve cross-functional collaboration

**Learning & Experimentation:**
- Integrate experimentation into standard workflows
- Measure learning metrics
- Optimize knowledge sharing
- Set goal: 85% of teams continuously experiment

### From Level 4 to Level 5

**Key Focus:** Innovate, lead, share externally

**Prototyping & Design Velocity:**
- Enable PMs and Product Designers to submit pull requests
- Train PMs and Product Designers to generate Python notebooks
- Create frameworks and templates for code contributions from non-engineering roles
- Set goal: PMs and Product Designers contributing code regularly

**Data & Insights Automation:**
- Build reusable analysis frameworks and libraries
- Create internal tools that democratize data analysis
- Enable self-service research analysis tools
- Contribute to open source AI analysis or research tools
- Set goal: Internal tools used by other teams, open source contributions

**All Dimensions:**
- Pioneer new use cases and techniques
- Contribute to tool development
- Share best practices externally (blog posts, talks)
- Build thought leadership
- Set goal: Industry recognition, competitive advantage

---

## Format Presentation Ideas

### Visual Format Options

1. **Matrix View** (Most Common)
   - Rows: Dimensions
   - Columns: Maturity Levels
   - Cells: Descriptions, indicators, or scores
   - Color gradient from low (red/orange) to high (green/blue)

2. **Spider/Radar Chart**
   - Each dimension as a spoke
   - Current state plotted as a shape
   - Target state as outer ring
   - Shows gaps visually

3. **Journey Map Format**
   - Horizontal progression through levels
   - Each level shows "what it looks like"
   - Clear "next steps" to advance

4. **Card-Based Assessment**
   - Each dimension as a card
   - Self-assessment checklist
   - Visual progress indicators
   - Action items per level

### Document Structure

1. **Cover/Introduction**
   - Model name and purpose
   - Who it's for
   - How to use it

2. **Overview Section**
   - What is AI-native maturity?
   - Why it matters
   - Model structure explanation

3. **Dimension Deep Dives**
   - Each dimension gets 2-3 pages
   - Level descriptions
   - Assessment questions
   - Examples and case studies

4. **Assessment Tool**
   - Self-assessment questionnaire
   - Scoring methodology
   - Results interpretation

5. **Roadmap/Next Steps**
   - How to advance from current level
   - Recommended initiatives per level
   - Resources and support

6. **Appendices**
   - Glossary
   - Tool recommendations
   - Training resources
   - Case studies

---

## Key Differentiators for AI Native Focus

Unlike generic AI maturity models, emphasize:

1. **Product Development Speed** - How quickly can you go from idea to prototype?
2. **Builder Culture** - Is experimentation celebrated and safe?
3. **Cross-Functional AI Use** - PM, Design, Data all using AI tools
4. **Prototype-to-Value** - Not just AI adoption, but impact on product outcomes
5. **Tool Fluency** - Not just having tools, but using them effectively
6. **Learning Velocity** - How fast does the org learn and adapt?

---

## Assessment Approach

### Self-Assessment Questions (Example Format)

For each dimension, provide 3-5 questions per level:

**Tool Adoption - Level 3 Example:**
- [ ] Do you have a standardized set of AI tools approved for product teams?
- [ ] Are AI tools integrated into your standard project kickoff templates?
- [ ] Do you offer quarterly training on AI tools?
- [ ] Is tool usage tracked and measured across teams?
- [ ] Can new team members easily access and learn approved tools?

**Scoring:**
- 0-2 Yes = Level 1
- 3-4 Yes = Level 2
- 5+ Yes = Level 3 (move to Level 4 questions)

---

## Next Steps for Development

1. **Review AEOMM PDF** - Extract exact format elements you want to preserve
2. **Define Dimensions** - Finalize the 5-7 key dimensions
3. **Write Level Descriptions** - Create detailed descriptions for each dimension Ã— level
4. **Create Assessment Tool** - Build the self-assessment questionnaire
5. **Design Visual Format** - Choose matrix, radar, or journey map style
6. **Add Examples** - Include real-world examples and case studies
7. **Test with Internal Team** - Validate the model with your own org first

---

## Questions to Consider

1. **Audience:** Is this for internal use, external sharing, or both?
2. **Depth:** How detailed should each level description be?
3. **Visual Style:** Should it match your Builder Day brand/aesthetic?
4. **Interactivity:** Will this be a static PDF or interactive tool?
5. **Updates:** How often will you refresh the model as AI evolves?

---

## Inspiration from Builder Day Context

Your Builder Day framework already includes:
- **Tracks** (Prototyping, Insights)
- **Categories** (Learning, Interaction Design, Workflow, Data, Creative)
- **Success Metrics** (Participation, Completion, Tool Adoption, Confidence)

Consider aligning maturity model dimensions with these existing frameworks for consistency.

